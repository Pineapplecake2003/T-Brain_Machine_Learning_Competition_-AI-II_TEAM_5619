{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Create Folders"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T17:57:42.038035Z","iopub.status.busy":"2024-05-23T17:57:42.037655Z","iopub.status.idle":"2024-05-23T17:57:45.220801Z","shell.execute_reply":"2024-05-23T17:57:45.219604Z","shell.execute_reply.started":"2024-05-23T17:57:42.038006Z"},"trusted":true},"outputs":[],"source":["!mkdir log\n","!mkdir model\n","!mkdir result"]},{"cell_type":"markdown","metadata":{},"source":["# Import"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T17:57:45.223824Z","iopub.status.busy":"2024-05-23T17:57:45.223411Z","iopub.status.idle":"2024-05-23T17:57:45.232928Z","shell.execute_reply":"2024-05-23T17:57:45.231753Z","shell.execute_reply.started":"2024-05-23T17:57:45.223787Z"},"trusted":true},"outputs":[],"source":["import gc\n","import time\n","import torch\n","import datetime\n","from utils import *\n","import torch.nn as nn\n","from Model import Unet\n","import torch.optim as optim\n","from Dataset import NavigationDataset\n","from torch.utils.data import DataLoader\n","from Loss import TverskyLoss, WeightedCrossEntropyLoss"]},{"cell_type":"markdown","metadata":{},"source":["# GLOBAL VARS"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T17:57:45.235051Z","iopub.status.busy":"2024-05-23T17:57:45.234286Z","iopub.status.idle":"2024-05-23T17:57:45.250251Z","shell.execute_reply":"2024-05-23T17:57:45.249335Z","shell.execute_reply.started":"2024-05-23T17:57:45.235019Z"},"trusted":true},"outputs":[],"source":["IMG_WIDTH = 428\n","IMG_HIGHT = 240\n","torch.cuda.empty_cache()\n","gc.collect()"]},{"cell_type":"markdown","metadata":{},"source":["# Train function"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T17:57:45.462243Z","iopub.status.busy":"2024-05-23T17:57:45.461896Z","iopub.status.idle":"2024-05-23T17:57:45.484760Z","shell.execute_reply":"2024-05-23T17:57:45.483796Z","shell.execute_reply.started":"2024-05-23T17:57:45.462213Z"},"trusted":true},"outputs":[],"source":["def train(model, inital_learning_rate, max_epoch, train_dataloader, vaild_dataloader):\n","    best_val_loss = float('inf')\n","    train_loss = []\n","    vaild_loss = []\n","    learning_rate = []\n","\n","    # optimizer\n","    optimizer = optim.Adamax(model.parameters(), inital_learning_rate, weight_decay=0.02)\n","    \n","    # loss function\n","    loss_tversky = TverskyLoss()\n","    loss_bce = WeightedCrossEntropyLoss(pos_weight = 0.65/0.35)\n","\n","    epoch = 0\n","    while epoch <= max_epoch:\n","        loss_all = 0\n","        epoch_step = 0\n","        train_loss_bce_record = 0\n","        train_loss_tversky_record = 0\n","        \n","        optimizer = lrfn(epoch, optimizer)\n","        learning_rate.append(optimizer.param_groups[0]['lr'])\n","        \n","        model.train()\n","        for i, data in enumerate(train_dataloader):\n","            images, ground_truths = data\n","            images, ground_truths = images.to(get_device()), ground_truths.to(get_device())\n","\n","            optimizer.zero_grad()\n","            outs, d2, d3, d4, b = model(images)\n","\n","            batch_loss = \\\n","                loss_bce(outs, ground_truths) + loss_bce(d2, ground_truths) + loss_bce(d3, ground_truths) + loss_bce(d4, ground_truths) + loss_bce(b, ground_truths) + \\\n","                loss_tversky(outs, ground_truths) + loss_tversky(d2, ground_truths) + loss_tversky(d3, ground_truths) + loss_tversky(d4, ground_truths) + loss_tversky(b, ground_truths)\n","                \n","            \n","            loss_bce_result = loss_bce(outs, ground_truths)\n","            loss_tversky_result = loss_tversky(outs, ground_truths)\n","\n","            del outs, d2, d3, d4, b \n","\n","            batch_loss.backward()\n","            optimizer.step()\n","\n","            epoch_step +=1\n","\n","            loss_all += batch_loss.item()\n","            \n","            train_loss_bce_record += loss_bce_result.item()\n","            train_loss_tversky_record += loss_tversky_result.item()\n","\n","\n","        train_avg_loss = loss_all / epoch_step\n","        \n","        train_loss_bce_record_avg = train_loss_bce_record / epoch_step\n","        train_loss_tversky_record_avg = train_loss_tversky_record / epoch_step\n","        \n","        train_loss.append(train_avg_loss)\n","\n","        model.eval()\n","        with torch.no_grad():\n","            val_loss_all = 0\n","            val_step = 0\n","            val_loss_bce_record = 0\n","            val_loss_tversky_record = 0\n","            \n","            for i, data in enumerate(vaild_dataloader):\n","                images, ground_truths = data\n","                images, ground_truths = images.to(get_device()), ground_truths.to(get_device())\n","\n","                outs, d2, d3, d4, b = model(images)\n","\n","                batch_loss = \\\n","                    loss_bce(outs, ground_truths) + loss_bce(d2, ground_truths) + loss_bce(d3, ground_truths) + loss_bce(d4, ground_truths) + loss_bce(b, ground_truths) + \\\n","                    loss_tversky(outs, ground_truths) + loss_tversky(d2, ground_truths) + loss_tversky(d3, ground_truths) + loss_tversky(d4, ground_truths) + loss_tversky(b, ground_truths)\n","            \n","                loss_bce_result = loss_bce(outs, ground_truths)\n","                loss_tversky_result = loss_tversky(outs, ground_truths)\n","                \n","                del outs, d2, d3, d4, b \n","\n","                val_loss_all += batch_loss.item()\n","                \n","                val_loss_bce_record += loss_bce_result.item()\n","                val_loss_tversky_record += loss_tversky_result.item()\n","                \n","                val_step += 1\n","\n","\n","            val_avg_loss = val_loss_all / val_step\n","            \n","            val_loss_bce_record_avg = val_loss_bce_record / val_step\n","            val_loss_tversky_record_avg = val_loss_tversky_record / val_step\n","            \n","            vaild_loss.append(val_avg_loss)\n","            \n","        print(f\"[{epoch+1:3d}/{max_epoch:3d}] learning rate: {optimizer.param_groups[0]['lr']}\")\n","        print(f\"training:\")\n","        print(f\"WBCE loss: {train_loss_bce_record_avg:.8f}, Tversky loss: {train_loss_tversky_record_avg:.8f}, total loss: {train_avg_loss:.8f}\")\n","        print(f\"validating:\")\n","        print(f\"WBCE loss: {val_loss_bce_record_avg:.8f}, Tversky loss: {val_loss_tversky_record_avg:.8f}, total loss: {val_avg_loss:.8f}\")\n","        print(\"====================================================================\")\n","\n","        if val_avg_loss < best_val_loss:\n","            best_val_loss = val_avg_loss\n","            print(\"Saving better model...\")\n","            torch.save(model.state_dict(), \"./model/best_model.pth\")\n","            print(\"Saving better model complete.\")\n","\n","        record_log(train_loss, vaild_loss, epoch, train_dataloader.batch_size, best_val_loss, learning_rate)\n","\n","        torch.save(model.state_dict(), \"./model/newest_model.pth\")\n","\n","        epoch +=1"]},{"cell_type":"markdown","metadata":{},"source":["# Main"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T17:57:45.486622Z","iopub.status.busy":"2024-05-23T17:57:45.486174Z","iopub.status.idle":"2024-05-23T18:01:54.975271Z","shell.execute_reply":"2024-05-23T18:01:54.973653Z","shell.execute_reply.started":"2024-05-23T17:57:45.486579Z"},"trusted":true},"outputs":[],"source":["def main():\n","    \n","    # hyper parameter\n","    batch_size = 50\n","    learning_rate = 0.00001\n","    max_epoch = 80\n","\n","    # dataset\n","    print(\"Loading training dataset...\")\n","    train_set = NavigationDataset(mode=\"TRAIN\", dataset_path=\"./Dataset/\")\n","\n","    # 10% of training set used as valid set\n","    valid_set_percent = 0.1\n","    vaild_set_length = int(len(train_set) * valid_set_percent)\n","\n","    train_set,valid_set = torch.utils.data.random_split(\n","        dataset = train_set, lengths = [len(train_set)-vaild_set_length, vaild_set_length],\n","        generator = torch.Generator().manual_seed(int(time.time()))\n","    )\n","\n","    train_dataloader = DataLoader(train_set, batch_size= batch_size, shuffle= True)\n","    valid_dataloader = DataLoader(valid_set, batch_size= batch_size, shuffle= True)\n","    print(\"Loading complete.\")\n","\n","    # record indices in each dataset\n","    with open(\"./log/train_set_log.txt\", \"w\") as f:\n","        for train_index in train_dataloader.dataset.indices:\n","            f.write(f\"{train_index},\")\n","        f.close()\n","\n","    with open(\"./log/valid_set_log.txt\", \"w\") as f:\n","        for vaild_index in valid_dataloader.dataset.indices:\n","            f.write(f\"{vaild_index},\")\n","        f.close()\n","    \n","    with open(\"./log/seed_log.txt\", \"w\") as f:\n","        f.write(f\"{torch.random.initial_seed()},\")\n","        f.close()\n","\n","    # model\n","    model = Unet(3, 1)\n","    model.to(get_device())\n","    model = nn.DataParallel(model, device_ids=[i for i in range(torch.cuda.device_count())])\n","\n","    # train\n","    print(\"Start training...\")\n","    start_time = datetime.datetime.now()\n","    train(model, learning_rate, max_epoch, train_dataloader, valid_dataloader)\n","    end_time = datetime.datetime.now()\n","    print(f\"Training complete.\\nCost {str(end_time - start_time)} to training.\")\n","    torch.cuda.empty_cache()\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"markdown","metadata":{},"source":["# Plot result"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","train_loss = []\n","with open(\"./log/\" + \"train_loss_log.txt\",\"r\") as f:\n","    train_loss = f.read().split(\",\")[:-1]\n","    for i in range(len(train_loss)):\n","        train_loss[i] = float(train_loss[i])\n","\n","vaild_loss = []\n","with open(\"./log/\" + \"vaild_loss_log.txt\",\"r\") as f:\n","    vaild_loss = f.read().split(\",\")[:-1]\n","    for i in range(len(vaild_loss)):\n","        vaild_loss[i] = float(vaild_loss[i])\n","\n","fig, axes = plt.subplots(1,1, figsize = (10,5))\n","\n","axes.grid(True)\n","axes.set_title(\"Loss per epoch\")\n","axes.set_xlabel(\"epoch\")\n","axes.set_ylabel(\"Loss\")\n","axes.plot(list(range(1,len(train_loss)+1)), train_loss,'b')\n","axes.plot(list(range(1,len(vaild_loss)+1)), vaild_loss,'r')\n","axes.legend([\"train\",\"vaild\"])\n","\n","plt.show()"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4984479,"sourceId":8381757,"sourceType":"datasetVersion"}],"dockerImageVersionId":30698,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":4}
